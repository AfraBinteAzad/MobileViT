{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMh_c8_TirHO",
        "outputId": "18748cb1-875f-48ad-85c0-edc3958ec2ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_14 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d_162 (Conv2D)         (None, 128, 128, 16)         432       ['input_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_206 (B  (None, 128, 128, 16)         64        ['conv2d_162[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_159 (Activation  (None, 128, 128, 16)         0         ['batch_normalization_206[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_163 (Conv2D)         (None, 128, 128, 64)         1024      ['activation_159[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_207 (B  (None, 128, 128, 64)         256       ['conv2d_163[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_160 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_207[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_47 (Depth  (None, 128, 128, 64)         576       ['activation_160[0][0]']      \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_208 (B  (None, 128, 128, 64)         256       ['depthwise_conv2d_47[0][0]'] \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_161 (Activation  (None, 128, 128, 64)         0         ['batch_normalization_208[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_164 (Conv2D)         (None, 128, 128, 32)         2048      ['activation_161[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_209 (B  (None, 128, 128, 32)         128       ['conv2d_164[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_165 (Conv2D)         (None, 128, 128, 128)        4096      ['batch_normalization_209[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " batch_normalization_210 (B  (None, 128, 128, 128)        512       ['conv2d_165[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_162 (Activation  (None, 128, 128, 128)        0         ['batch_normalization_210[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_48 (Depth  (None, 64, 64, 128)          1152      ['activation_162[0][0]']      \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_211 (B  (None, 64, 64, 128)          512       ['depthwise_conv2d_48[0][0]'] \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_163 (Activation  (None, 64, 64, 128)          0         ['batch_normalization_211[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_166 (Conv2D)         (None, 64, 64, 64)           8192      ['activation_163[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_212 (B  (None, 64, 64, 64)           256       ['conv2d_166[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_167 (Conv2D)         (None, 64, 64, 256)          16384     ['batch_normalization_212[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " batch_normalization_213 (B  (None, 64, 64, 256)          1024      ['conv2d_167[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_164 (Activation  (None, 64, 64, 256)          0         ['batch_normalization_213[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_49 (Depth  (None, 64, 64, 256)          2304      ['activation_164[0][0]']      \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_214 (B  (None, 64, 64, 256)          1024      ['depthwise_conv2d_49[0][0]'] \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_165 (Activation  (None, 64, 64, 256)          0         ['batch_normalization_214[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_168 (Conv2D)         (None, 64, 64, 64)           16384     ['activation_165[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_215 (B  (None, 64, 64, 64)           256       ['conv2d_168[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_86 (Add)                (None, 64, 64, 64)           0         ['batch_normalization_212[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_215[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_169 (Conv2D)         (None, 64, 64, 256)          16384     ['add_86[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_216 (B  (None, 64, 64, 256)          1024      ['conv2d_169[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_166 (Activation  (None, 64, 64, 256)          0         ['batch_normalization_216[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_50 (Depth  (None, 64, 64, 256)          2304      ['activation_166[0][0]']      \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_217 (B  (None, 64, 64, 256)          1024      ['depthwise_conv2d_50[0][0]'] \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_167 (Activation  (None, 64, 64, 256)          0         ['batch_normalization_217[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_170 (Conv2D)         (None, 64, 64, 64)           16384     ['activation_167[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_218 (B  (None, 64, 64, 64)           256       ['conv2d_170[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_87 (Add)                (None, 64, 64, 64)           0         ['add_86[0][0]',              \n",
            "                                                                     'batch_normalization_218[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_171 (Conv2D)         (None, 64, 64, 256)          16384     ['add_87[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_219 (B  (None, 64, 64, 256)          1024      ['conv2d_171[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_168 (Activation  (None, 64, 64, 256)          0         ['batch_normalization_219[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_51 (Depth  (None, 32, 32, 256)          2304      ['activation_168[0][0]']      \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_220 (B  (None, 32, 32, 256)          1024      ['depthwise_conv2d_51[0][0]'] \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_169 (Activation  (None, 32, 32, 256)          0         ['batch_normalization_220[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_172 (Conv2D)         (None, 32, 32, 96)           24576     ['activation_169[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_221 (B  (None, 32, 32, 96)           384       ['conv2d_172[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_173 (Conv2D)         (None, 32, 32, 96)           82944     ['batch_normalization_221[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " batch_normalization_222 (B  (None, 32, 32, 96)           384       ['conv2d_173[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_170 (Activation  (None, 32, 32, 96)           0         ['batch_normalization_222[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_174 (Conv2D)         (None, 32, 32, 144)          13824     ['activation_170[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_223 (B  (None, 32, 32, 144)          576       ['conv2d_174[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_171 (Activation  (None, 32, 32, 144)          0         ['batch_normalization_223[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " reshape_26 (Reshape)        (None, 4, 256, 144)          0         ['activation_171[0][0]']      \n",
            "                                                                                                  \n",
            " layer_normalization_70 (La  (None, 4, 256, 144)          288       ['reshape_26[0][0]']          \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_35 (M  (None, 4, 256, 144)          83520     ['layer_normalization_70[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_70[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_88 (Add)                (None, 4, 256, 144)          0         ['multi_head_attention_35[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'reshape_26[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_71 (La  (None, 4, 256, 144)          288       ['add_88[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_72 (Dense)            (None, 4, 256, 288)          41760     ['layer_normalization_71[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_70 (Dropout)        (None, 4, 256, 288)          0         ['dense_72[0][0]']            \n",
            "                                                                                                  \n",
            " dense_73 (Dense)            (None, 4, 256, 144)          41616     ['dropout_70[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_71 (Dropout)        (None, 4, 256, 144)          0         ['dense_73[0][0]']            \n",
            "                                                                                                  \n",
            " add_89 (Add)                (None, 4, 256, 144)          0         ['dropout_71[0][0]',          \n",
            "                                                                     'add_88[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_72 (La  (None, 4, 256, 144)          288       ['add_89[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_36 (M  (None, 4, 256, 144)          83520     ['layer_normalization_72[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_72[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_90 (Add)                (None, 4, 256, 144)          0         ['multi_head_attention_36[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'add_89[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_73 (La  (None, 4, 256, 144)          288       ['add_90[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_74 (Dense)            (None, 4, 256, 288)          41760     ['layer_normalization_73[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_72 (Dropout)        (None, 4, 256, 288)          0         ['dense_74[0][0]']            \n",
            "                                                                                                  \n",
            " dense_75 (Dense)            (None, 4, 256, 144)          41616     ['dropout_72[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_73 (Dropout)        (None, 4, 256, 144)          0         ['dense_75[0][0]']            \n",
            "                                                                                                  \n",
            " add_91 (Add)                (None, 4, 256, 144)          0         ['dropout_73[0][0]',          \n",
            "                                                                     'add_90[0][0]']              \n",
            "                                                                                                  \n",
            " reshape_27 (Reshape)        (None, 32, 32, 144)          0         ['add_91[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_175 (Conv2D)         (None, 32, 32, 96)           13824     ['reshape_27[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_224 (B  (None, 32, 32, 96)           384       ['conv2d_175[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_172 (Activation  (None, 32, 32, 96)           0         ['batch_normalization_224[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenat  (None, 32, 32, 192)          0         ['activation_172[0][0]',      \n",
            " e)                                                                  'batch_normalization_221[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_176 (Conv2D)         (None, 32, 32, 144)          248832    ['concatenate_13[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_225 (B  (None, 32, 32, 144)          576       ['conv2d_176[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_173 (Activation  (None, 32, 32, 144)          0         ['batch_normalization_225[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_177 (Conv2D)         (None, 32, 32, 576)          82944     ['activation_173[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_226 (B  (None, 32, 32, 576)          2304      ['conv2d_177[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_174 (Activation  (None, 32, 32, 576)          0         ['batch_normalization_226[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_52 (Depth  (None, 16, 16, 576)          5184      ['activation_174[0][0]']      \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_227 (B  (None, 16, 16, 576)          2304      ['depthwise_conv2d_52[0][0]'] \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_175 (Activation  (None, 16, 16, 576)          0         ['batch_normalization_227[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_178 (Conv2D)         (None, 16, 16, 128)          73728     ['activation_175[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_228 (B  (None, 16, 16, 128)          512       ['conv2d_178[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_179 (Conv2D)         (None, 16, 16, 128)          147456    ['batch_normalization_228[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " batch_normalization_229 (B  (None, 16, 16, 128)          512       ['conv2d_179[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_176 (Activation  (None, 16, 16, 128)          0         ['batch_normalization_229[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_180 (Conv2D)         (None, 16, 16, 192)          24576     ['activation_176[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_230 (B  (None, 16, 16, 192)          768       ['conv2d_180[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_177 (Activation  (None, 16, 16, 192)          0         ['batch_normalization_230[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " reshape_28 (Reshape)        (None, 4, 64, 192)           0         ['activation_177[0][0]']      \n",
            "                                                                                                  \n",
            " layer_normalization_74 (La  (None, 4, 64, 192)           384       ['reshape_28[0][0]']          \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_37 (M  (None, 4, 64, 192)           148224    ['layer_normalization_74[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_74[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_92 (Add)                (None, 4, 64, 192)           0         ['multi_head_attention_37[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'reshape_28[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_75 (La  (None, 4, 64, 192)           384       ['add_92[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_76 (Dense)            (None, 4, 64, 384)           74112     ['layer_normalization_75[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_74 (Dropout)        (None, 4, 64, 384)           0         ['dense_76[0][0]']            \n",
            "                                                                                                  \n",
            " dense_77 (Dense)            (None, 4, 64, 192)           73920     ['dropout_74[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)        (None, 4, 64, 192)           0         ['dense_77[0][0]']            \n",
            "                                                                                                  \n",
            " add_93 (Add)                (None, 4, 64, 192)           0         ['dropout_75[0][0]',          \n",
            "                                                                     'add_92[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_76 (La  (None, 4, 64, 192)           384       ['add_93[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_38 (M  (None, 4, 64, 192)           148224    ['layer_normalization_76[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_76[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_94 (Add)                (None, 4, 64, 192)           0         ['multi_head_attention_38[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'add_93[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_77 (La  (None, 4, 64, 192)           384       ['add_94[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_78 (Dense)            (None, 4, 64, 384)           74112     ['layer_normalization_77[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)        (None, 4, 64, 384)           0         ['dense_78[0][0]']            \n",
            "                                                                                                  \n",
            " dense_79 (Dense)            (None, 4, 64, 192)           73920     ['dropout_76[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)        (None, 4, 64, 192)           0         ['dense_79[0][0]']            \n",
            "                                                                                                  \n",
            " add_95 (Add)                (None, 4, 64, 192)           0         ['dropout_77[0][0]',          \n",
            "                                                                     'add_94[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_78 (La  (None, 4, 64, 192)           384       ['add_95[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (M  (None, 4, 64, 192)           148224    ['layer_normalization_78[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_78[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_96 (Add)                (None, 4, 64, 192)           0         ['multi_head_attention_39[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'add_95[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_79 (La  (None, 4, 64, 192)           384       ['add_96[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_80 (Dense)            (None, 4, 64, 384)           74112     ['layer_normalization_79[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_78 (Dropout)        (None, 4, 64, 384)           0         ['dense_80[0][0]']            \n",
            "                                                                                                  \n",
            " dense_81 (Dense)            (None, 4, 64, 192)           73920     ['dropout_78[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_79 (Dropout)        (None, 4, 64, 192)           0         ['dense_81[0][0]']            \n",
            "                                                                                                  \n",
            " add_97 (Add)                (None, 4, 64, 192)           0         ['dropout_79[0][0]',          \n",
            "                                                                     'add_96[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_80 (La  (None, 4, 64, 192)           384       ['add_97[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (M  (None, 4, 64, 192)           148224    ['layer_normalization_80[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_80[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_98 (Add)                (None, 4, 64, 192)           0         ['multi_head_attention_40[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'add_97[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_81 (La  (None, 4, 64, 192)           384       ['add_98[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_82 (Dense)            (None, 4, 64, 384)           74112     ['layer_normalization_81[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_80 (Dropout)        (None, 4, 64, 384)           0         ['dense_82[0][0]']            \n",
            "                                                                                                  \n",
            " dense_83 (Dense)            (None, 4, 64, 192)           73920     ['dropout_80[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_81 (Dropout)        (None, 4, 64, 192)           0         ['dense_83[0][0]']            \n",
            "                                                                                                  \n",
            " add_99 (Add)                (None, 4, 64, 192)           0         ['dropout_81[0][0]',          \n",
            "                                                                     'add_98[0][0]']              \n",
            "                                                                                                  \n",
            " reshape_29 (Reshape)        (None, 16, 16, 192)          0         ['add_99[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_181 (Conv2D)         (None, 16, 16, 128)          24576     ['reshape_29[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_231 (B  (None, 16, 16, 128)          512       ['conv2d_181[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_178 (Activation  (None, 16, 16, 128)          0         ['batch_normalization_231[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenat  (None, 16, 16, 256)          0         ['activation_178[0][0]',      \n",
            " e)                                                                  'batch_normalization_228[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_182 (Conv2D)         (None, 16, 16, 192)          442368    ['concatenate_14[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_232 (B  (None, 16, 16, 192)          768       ['conv2d_182[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_179 (Activation  (None, 16, 16, 192)          0         ['batch_normalization_232[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_183 (Conv2D)         (None, 16, 16, 768)          147456    ['activation_179[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_233 (B  (None, 16, 16, 768)          3072      ['conv2d_183[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_180 (Activation  (None, 16, 16, 768)          0         ['batch_normalization_233[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_53 (Depth  (None, 8, 8, 768)            6912      ['activation_180[0][0]']      \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_234 (B  (None, 8, 8, 768)            3072      ['depthwise_conv2d_53[0][0]'] \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_181 (Activation  (None, 8, 8, 768)            0         ['batch_normalization_234[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_184 (Conv2D)         (None, 8, 8, 160)            122880    ['activation_181[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_235 (B  (None, 8, 8, 160)            640       ['conv2d_184[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_185 (Conv2D)         (None, 8, 8, 160)            230400    ['batch_normalization_235[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " batch_normalization_236 (B  (None, 8, 8, 160)            640       ['conv2d_185[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_182 (Activation  (None, 8, 8, 160)            0         ['batch_normalization_236[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_186 (Conv2D)         (None, 8, 8, 240)            38400     ['activation_182[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_237 (B  (None, 8, 8, 240)            960       ['conv2d_186[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_183 (Activation  (None, 8, 8, 240)            0         ['batch_normalization_237[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " reshape_30 (Reshape)        (None, 4, 16, 240)           0         ['activation_183[0][0]']      \n",
            "                                                                                                  \n",
            " layer_normalization_82 (La  (None, 4, 16, 240)           480       ['reshape_30[0][0]']          \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (M  (None, 4, 16, 240)           231360    ['layer_normalization_82[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_82[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_100 (Add)               (None, 4, 16, 240)           0         ['multi_head_attention_41[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'reshape_30[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_83 (La  (None, 4, 16, 240)           480       ['add_100[0][0]']             \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_84 (Dense)            (None, 4, 16, 480)           115680    ['layer_normalization_83[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_82 (Dropout)        (None, 4, 16, 480)           0         ['dense_84[0][0]']            \n",
            "                                                                                                  \n",
            " dense_85 (Dense)            (None, 4, 16, 240)           115440    ['dropout_82[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_83 (Dropout)        (None, 4, 16, 240)           0         ['dense_85[0][0]']            \n",
            "                                                                                                  \n",
            " add_101 (Add)               (None, 4, 16, 240)           0         ['dropout_83[0][0]',          \n",
            "                                                                     'add_100[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_84 (La  (None, 4, 16, 240)           480       ['add_101[0][0]']             \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_42 (M  (None, 4, 16, 240)           231360    ['layer_normalization_84[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_84[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_102 (Add)               (None, 4, 16, 240)           0         ['multi_head_attention_42[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'add_101[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_85 (La  (None, 4, 16, 240)           480       ['add_102[0][0]']             \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_86 (Dense)            (None, 4, 16, 480)           115680    ['layer_normalization_85[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_84 (Dropout)        (None, 4, 16, 480)           0         ['dense_86[0][0]']            \n",
            "                                                                                                  \n",
            " dense_87 (Dense)            (None, 4, 16, 240)           115440    ['dropout_84[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_85 (Dropout)        (None, 4, 16, 240)           0         ['dense_87[0][0]']            \n",
            "                                                                                                  \n",
            " add_103 (Add)               (None, 4, 16, 240)           0         ['dropout_85[0][0]',          \n",
            "                                                                     'add_102[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_86 (La  (None, 4, 16, 240)           480       ['add_103[0][0]']             \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_43 (M  (None, 4, 16, 240)           231360    ['layer_normalization_86[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_86[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_104 (Add)               (None, 4, 16, 240)           0         ['multi_head_attention_43[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'add_103[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_87 (La  (None, 4, 16, 240)           480       ['add_104[0][0]']             \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_88 (Dense)            (None, 4, 16, 480)           115680    ['layer_normalization_87[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_86 (Dropout)        (None, 4, 16, 480)           0         ['dense_88[0][0]']            \n",
            "                                                                                                  \n",
            " dense_89 (Dense)            (None, 4, 16, 240)           115440    ['dropout_86[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_87 (Dropout)        (None, 4, 16, 240)           0         ['dense_89[0][0]']            \n",
            "                                                                                                  \n",
            " add_105 (Add)               (None, 4, 16, 240)           0         ['dropout_87[0][0]',          \n",
            "                                                                     'add_104[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_31 (Reshape)        (None, 8, 8, 240)            0         ['add_105[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_187 (Conv2D)         (None, 8, 8, 160)            38400     ['reshape_31[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_238 (B  (None, 8, 8, 160)            640       ['conv2d_187[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_184 (Activation  (None, 8, 8, 160)            0         ['batch_normalization_238[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenat  (None, 8, 8, 320)            0         ['activation_184[0][0]',      \n",
            " e)                                                                  'batch_normalization_235[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_188 (Conv2D)         (None, 8, 8, 240)            691200    ['concatenate_15[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_239 (B  (None, 8, 8, 240)            960       ['conv2d_188[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_185 (Activation  (None, 8, 8, 240)            0         ['batch_normalization_239[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_189 (Conv2D)         (None, 8, 8, 640)            153600    ['activation_185[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_240 (B  (None, 8, 8, 640)            2560      ['conv2d_189[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_186 (Activation  (None, 8, 8, 640)            0         ['batch_normalization_240[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2  (None, 640)                  0         ['activation_186[0][0]']      \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " dense_90 (Dense)            (None, 1000)                 641000    ['global_average_pooling2d_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6305960 (24.06 MB)\n",
            "Trainable params: 6290376 (24.00 MB)\n",
            "Non-trainable params: 15584 (60.88 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from typing_extensions import Concatenate\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers as L\n",
        "\n",
        "def inverted_residual_block(inputs, num_of_filters,strides=1,expansion_ratio=1):\n",
        "  ##point-wise convulation\n",
        "  x=L.Conv2D(\n",
        "      filters= expansion_ratio*inputs.shape[-1],\n",
        "      kernel_size=1,\n",
        "      padding=\"same\",\n",
        "      use_bias=False\n",
        "      )(inputs)\n",
        "\n",
        "  x=L.BatchNormalization()(x)\n",
        "  x=L.Activation(\"swish\")(x)\n",
        "\n",
        " ##depth-wise convulation\n",
        "\n",
        "  x=L.DepthwiseConv2D(\n",
        "      kernel_size=3,\n",
        "      strides=strides,\n",
        "      padding=\"same\",\n",
        "      use_bias=False\n",
        "  )(x)\n",
        "  x=L.BatchNormalization()(x)\n",
        "  x=L.Activation(\"swish\")(x)\n",
        "\n",
        "\n",
        " ##point-wise convulation\n",
        "  x=L.Conv2D(\n",
        "      filters= num_of_filters,\n",
        "      kernel_size=1,\n",
        "      padding=\"same\",\n",
        "      use_bias=False\n",
        "  )(x)\n",
        "  x=L.BatchNormalization()(x)\n",
        "\n",
        " ##residual connection\n",
        "\n",
        "  if strides==1 and (inputs.shape==x.shape):\n",
        "      return L.Add()([inputs,x])\n",
        "  return x\n",
        "\n",
        "def mlp(x,mlp_d,d,dropout_rate=0.1):\n",
        "  x=L.Dense(mlp_d,activation=\"swish\")(x)\n",
        "  x=L.Dropout(dropout_rate)(x)\n",
        "  x=L.Dense(d)(x)\n",
        "  x=L.Dropout(dropout_rate)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def transformer_encoder(x,num_heads,d,mlp_d):\n",
        "  skip_1=x\n",
        "  x=L.LayerNormalization()(x)\n",
        "  x=L.MultiHeadAttention(\n",
        "      num_heads=num_heads,key_dim=d\n",
        "  )(x,x)\n",
        "  x=L.Add()([x,skip_1])\n",
        "\n",
        "  skip_2=x\n",
        "  x=L.LayerNormalization()(x)\n",
        "  x=mlp(x,mlp_d,d)\n",
        "  x=L.Add()([x,skip_2])\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def mobile_vit_block(inputs,num_filters,dimension,patch_size=2,num_layers=1):\n",
        "\n",
        "  ## B=Batch, H=height, W=Weight, C=Number of channels. B,H,W,C denotes the shape of input tensor\n",
        "\n",
        "  B,H,W,C=inputs.shape\n",
        "\n",
        "  ## 3x3 convolution\n",
        "  x=L.Conv2D(\n",
        "      filters=C,\n",
        "      kernel_size=3,\n",
        "      padding=\"same\",\n",
        "      use_bias=False\n",
        "  )(inputs)\n",
        "  x=L.BatchNormalization()(x)\n",
        "  x=L.Activation(\"swish\")(x)\n",
        "\n",
        "  ## 1x1 convolution\n",
        "  x=L.Conv2D(\n",
        "      filters=dimension,\n",
        "      kernel_size=1,\n",
        "      padding=\"same\",\n",
        "      use_bias=False\n",
        "  )(x)\n",
        "  x=L.BatchNormalization()(x)\n",
        "  x=L.Activation(\"swish\")(x)\n",
        "\n",
        "\n",
        "  ## Reshape x to flattened patches\n",
        "  P=patch_size*patch_size\n",
        "  N=int(H*W//P)\n",
        "  x=L.Reshape((P,N,dimension))(x)\n",
        "\n",
        "  ## transformer encoder\n",
        "  for _ in range(num_layers):\n",
        "    x=transformer_encoder(x,1,dimension,dimension*2)\n",
        "\n",
        "  ## reshape it back\n",
        "  x=L.Reshape((H,W,dimension))(x)\n",
        "\n",
        "\n",
        "  ## 1x1 convolution\n",
        "  x=L.Conv2D(\n",
        "      filters=C,\n",
        "      kernel_size=1,\n",
        "      padding=\"same\",\n",
        "      use_bias=False\n",
        "  )(x)\n",
        "  x=L.BatchNormalization()(x)\n",
        "  x=L.Activation(\"swish\")(x)\n",
        "\n",
        "\n",
        "  ## Concatenate\n",
        "  x=L.Concatenate()([x,inputs])\n",
        "\n",
        "\n",
        "  ## 3x3 convolution\n",
        "  x=L.Conv2D(\n",
        "      filters=num_filters,\n",
        "      kernel_size=3,\n",
        "      padding=\"same\",\n",
        "      use_bias=False\n",
        "  )(x)\n",
        "  x=L.BatchNormalization()(x)\n",
        "  x=L.Activation(\"swish\")(x)\n",
        "  return x\n",
        "\n",
        "def MobileViT(input_shape,num_channels,d,expansion_ratio,num_layers=[2,4,3],num_classes=1000):\n",
        "    ## Input layer\n",
        "    inputs=L.Input(input_shape)\n",
        "\n",
        "    ## Stem\n",
        "    x=L.Conv2D(\n",
        "        filters=num_channels[0],\n",
        "        kernel_size=3,\n",
        "        strides=2,\n",
        "        padding=\"same\",\n",
        "        use_bias=False\n",
        "    )(inputs)\n",
        "\n",
        "    x=L.BatchNormalization()(x)\n",
        "    x=L.Activation(\"swish\")(x)\n",
        "    x=inverted_residual_block(x,num_channels[1],strides=1,expansion_ratio=expansion_ratio)\n",
        "\n",
        "    ## Stage 1\n",
        "    x=inverted_residual_block(x,num_channels[2],strides=2,expansion_ratio=expansion_ratio)\n",
        "    x=inverted_residual_block(x,num_channels[3],strides=1,expansion_ratio=expansion_ratio)\n",
        "    x=inverted_residual_block(x,num_channels[4],strides=1,expansion_ratio=expansion_ratio)\n",
        "\n",
        "\n",
        "\n",
        "    ## Stage 2\n",
        "    x=inverted_residual_block(x,num_channels[5],strides=2,expansion_ratio=expansion_ratio)\n",
        "    x=mobile_vit_block(x,num_channels[6],d[0],num_layers=num_layers[0])\n",
        "\n",
        "\n",
        "    ## Stage 3\n",
        "    x=inverted_residual_block(x,num_channels[7],strides=2,expansion_ratio=expansion_ratio)\n",
        "    x=mobile_vit_block(x,num_channels[8],d[1],num_layers=num_layers[1])\n",
        "\n",
        "\n",
        "    ## Stage 4\n",
        "    x=inverted_residual_block(x,num_channels[9],strides=2,expansion_ratio=expansion_ratio)\n",
        "    x=mobile_vit_block(x,num_channels[10],d[2],num_layers=num_layers[2])\n",
        "    x=L.Conv2D(\n",
        "        filters=num_channels[11],\n",
        "        kernel_size=1,\n",
        "        padding=\"same\",\n",
        "        use_bias=False\n",
        "        )(x)\n",
        "    x=L.BatchNormalization()(x)\n",
        "    x=L.Activation(\"swish\")(x)\n",
        "\n",
        "    ## Classifier\n",
        "    x=L.GlobalAveragePooling2D()(x)\n",
        "    outputs=L.Dense(num_classes,activation=\"softmax\")(x)\n",
        "\n",
        "    model=tf.keras.models.Model(inputs,outputs)\n",
        "    return model\n",
        "def MobileViT_s(input_shape,num_classes):\n",
        "  num_channels=[16,32,64,64,64,96,144,128,192,160,240,640]\n",
        "  d=[144,192,240]\n",
        "  expansion_ratio=4\n",
        "  return MobileViT(\n",
        "      input_shape,\n",
        "      num_channels,\n",
        "      d,\n",
        "      expansion_ratio,\n",
        "      num_classes=num_classes\n",
        "  )\n",
        "\n",
        "def MobileViT_xs(input_shape,num_classes):\n",
        "  num_channels=[16,32,48,48,48,64,96,80,120,96,144,384]\n",
        "  d=[96,120,144]\n",
        "  expansion_ratio=4\n",
        "  return MobileViT(\n",
        "      input_shape,\n",
        "      num_channels,\n",
        "      d,\n",
        "      expansion_ratio,\n",
        "      num_classes=num_classes\n",
        "\n",
        "  )\n",
        "\n",
        "def MobileViT_xxs(input_shape,num_classes):\n",
        "  num_channels=[16,32,24,24,24,48,64,64,80,80,96,320]\n",
        "  d=[64,80,96]\n",
        "  expansion_ratio=2\n",
        "  return MobileViT(\n",
        "      input_shape,\n",
        "      num_channels,\n",
        "      d,\n",
        "      expansion_ratio,\n",
        "      num_classes=num_classes\n",
        "\n",
        "  )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  model=MobileViT_s((256,256,3),1000)\n",
        "  model.summary()\n",
        "\n"
      ]
    }
  ]
}